{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert a Python list into a PySpark DataFrame, use spark.createDataFrame() or convert it to an RDD first. Here's how:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ListToDataFrame\").getOrCreate()\n",
    "\n",
    "# Python list\n",
    "ls = [1, 2, 3, 5, 4, 7]\n",
    "\n",
    "# Convert each element to a list (for row structure)\n",
    "ls2 = [[i] for i in ls]\n",
    "\n",
    "# Correct way: Pass ls2 directly\n",
    "df = spark.createDataFrame(ls2, [\"number\"])\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert the dictionary d = {1: \"Tom\", 2: \"Brad\", 3: \"Joe\"} into a PySpark DataFrame, follow these steps:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DictToDataFrame\").getOrCreate()\n",
    "\n",
    "# Dictionary\n",
    "d = {1: \"Tom\", 2: \"Brad\", 3: \"Joe\"}\n",
    "\n",
    "# Convert dictionary to list of tuples\n",
    "ls = list(d.items())  # [(1, 'Tom'), (2, 'Brad'), (3, 'Joe')]\n",
    "\n",
    "# Create DataFrame with column names\n",
    "df = spark.createDataFrame(ls, [\"ID\", \"Name\"])\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert a list of tuples like ls = [(1, 3), (1, 4), (1, 5)] into a PySpark DataFrame, define column names during the conversion.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"TupleListToDataFrame\").getOrCreate()\n",
    "\n",
    "# List of tuples\n",
    "ls = [(1, 3), (1, 4), (1, 5)]\n",
    "\n",
    "# Convert list to DataFrame with column names\n",
    "df = spark.createDataFrame(ls, [\"col1\", \"col2\"])\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat_ws, when\n",
    "\n",
    "# Initialize Spark session (not needed in Databricks notebooks)\n",
    "spark = SparkSession.builder.appName(\"DatabricksETL\").getOrCreate()\n",
    "\n",
    "# Define file paths (modify according to your storage location)\n",
    "parquet_path = \"dbfs:/mnt/data/input.parquet\"\n",
    "json_path = \"dbfs:/mnt/data/input.json\"\n",
    "delta_table_path = \"dbfs:/mnt/data/output_delta\"\n",
    "\n",
    "# Read Parquet file\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Read JSON file\n",
    "json_df = spark.read.json(json_path)\n",
    "\n",
    "# ---- Step 1: Join both DataFrames on a common key (assuming 'id' exists in both) ----\n",
    "df = parquet_df.join(json_df, on=\"id\", how=\"inner\")\n",
    "\n",
    "# ---- Step 2: Add a new column combining name and address ----\n",
    "df = df.withColumn(\"full_details\", concat_ws(\", \", col(\"name\"), col(\"address\")))\n",
    "\n",
    "# ---- Step 3: Apply a transformation to modify the 'age' column ----\n",
    "df = df.withColumn(\"age_group\", when(col(\"age\") < 18, \"Minor\")\n",
    "                                  .when((col(\"age\") >= 18) & (col(\"age\") < 60), \"Adult\")\n",
    "                                  .otherwise(\"Senior\"))\n",
    "\n",
    "# ---- Step 4: Add a static column to indicate the processing batch ----\n",
    "df = df.withColumn(\"batch_date\", lit(\"2025-03-30\"))\n",
    "\n",
    "# Write the transformed DataFrame as a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Optionally, create a Delta table if needed\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS processed_data\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_table_path}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data processing completed and saved as Delta table.\")\n",
    "\n",
    "'''\n",
    "\n",
    "MERGE INTO target_table AS target\n",
    "USING source_table AS source\n",
    "ON target.id = source.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.name = source.name, target.age = source.age\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (id, name, age) VALUES (source.id, source.name, source.age);\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SeniorDE_Interview\").getOrCreate()\n",
    "\n",
    "# File paths (modify as needed)\n",
    "parquet_path = \"dbfs:/mnt/data/customer_data.parquet\"\n",
    "csv_path = \"dbfs:/mnt/data/transactions.csv\"\n",
    "delta_output_path = \"dbfs:/mnt/data/output_delta\"\n",
    "\n",
    "# Read Parquet (Customer Data)\n",
    "customer_df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Read CSV (Transactions Data)\n",
    "txn_df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "\n",
    "# Convert txn_date to date type\n",
    "txn_df = txn_df.withColumn(\"txn_date\", col(\"txn_date\").cast(\"date\"))\n",
    "\n",
    "# Join on customer_id\n",
    "df = customer_df.join(txn_df, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Filter transactions with amount > 1000\n",
    "df = df.filter(col(\"amount\") > 1000)\n",
    "\n",
    "# Extract txn_year from txn_date\n",
    "df = df.withColumn(\"txn_year\", year(col(\"txn_date\")))\n",
    "\n",
    "# Write output as partitioned Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"txn_year\").save(delta_output_path)\n",
    "\n",
    "print(\"Processing completed. Data written as Delta table.\")\n",
    "\n",
    "#############################################################\n",
    "# Spark SQL based\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SeniorDE_Interview\").getOrCreate()\n",
    "\n",
    "# File paths\n",
    "parquet_path = \"dbfs:/mnt/data/customer_data.parquet\"\n",
    "csv_path = \"dbfs:/mnt/data/transactions.csv\"\n",
    "delta_output_path = \"dbfs:/mnt/data/output_delta\"\n",
    "\n",
    "# Read Parquet (Customer Data) and create a temp view\n",
    "customer_df = spark.read.parquet(parquet_path)\n",
    "customer_df.createOrReplaceTempView(\"customer_data\")\n",
    "\n",
    "# Read CSV (Transactions Data) and create a temp view\n",
    "txn_df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "txn_df.createOrReplaceTempView(\"transactions_data\")\n",
    "\n",
    "# Convert txn_date to DATE format and create another view\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW transactions_cleaned AS\n",
    "    SELECT txn_id, customer_id, amount, CAST(txn_date AS DATE) AS txn_date\n",
    "    FROM transactions_data\n",
    "\"\"\")\n",
    "\n",
    "# SQL Query for Transformation\n",
    "final_df = spark.sql(\"\"\"\n",
    "    WITH joined_data AS (\n",
    "        SELECT c.customer_id, c.name, c.age, c.city, \n",
    "               t.txn_id, t.amount, t.txn_date,\n",
    "               YEAR(t.txn_date) AS txn_year\n",
    "        FROM customer_data c\n",
    "        INNER JOIN transactions_cleaned t ON c.customer_id = t.customer_id\n",
    "        WHERE t.amount > 1000\n",
    "    )\n",
    "    SELECT * FROM joined_data\n",
    "\"\"\")\n",
    "\n",
    "# Write the final DataFrame as a partitioned Delta table\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"txn_year\").save(delta_output_path)\n",
    "\n",
    "print(\"Processing completed. Data written as a Delta table.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
